<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-CN">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="命令词," />










<meta name="description" content="使用Google Speech Commands dataset的论文 Google speech commands dataset：谷歌开源唤醒词数据集，有6万条，30个唤醒词，每条音频长度1s Speech commands dataset v1  下载地址：http:&#x2F;&#x2F;download.tensorflow.org&#x2F;data&#x2F;speech_commands_v0.01.tar.gz   S">
<meta property="og:type" content="article">
<meta property="og:title" content="命令词论文笔记（二）使用Google Speech Commands dataset的论文">
<meta property="og:url" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/index.html">
<meta property="og:site_name" content="yelong的博客">
<meta property="og:description" content="使用Google Speech Commands dataset的论文 Google speech commands dataset：谷歌开源唤醒词数据集，有6万条，30个唤醒词，每条音频长度1s Speech commands dataset v1  下载地址：http:&#x2F;&#x2F;download.tensorflow.org&#x2F;data&#x2F;speech_commands_v0.01.tar.gz   S">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210820000810717.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210820000826516.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210608174746396.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210608174813294.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/20191017213712901.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/20191017214108129.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/20191017215828482.jpg">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210824101219809.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210824101255801.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210824101358185.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210824101413415.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/12976336-3796810af0e64f2b.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/12976336-003886915241671f.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/20200414190510809.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210824110540981.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210824110631141.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/JkoL7t.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104100737871.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104100921735.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104100945713.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104101011948.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104101029737.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104101052521.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104101107772.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104101142998.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104111436716.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104145324460.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220105142223167.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220105142911152.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220105142956043.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220331115729019.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220331115825729.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220331123332076.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220331123319620.png">
<meta property="og:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220426151746370.png">
<meta property="article:published_time" content="2021-06-28T16:00:00.000Z">
<meta property="article:modified_time" content="2022-11-15T07:50:07.152Z">
<meta property="article:author" content="Long Ye">
<meta property="article:tag" content="命令词">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210820000810717.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '',
    scheme: 'Muse',
    version: '5.1.4',
    sidebar: {"position":"left","display":"hide","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://example.com/2021/06/29/命令词/命令词论文笔记（二）/"/>





  <title>命令词论文笔记（二）使用Google Speech Commands dataset的论文 | yelong的博客</title>
  








<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">yelong的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>




 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="yelong的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">命令词论文笔记（二）使用Google Speech Commands dataset的论文</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2021-06-29T00:00:00+08:00">
                2021-06-29
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%91%BD%E4%BB%A4%E8%AF%8D/" itemprop="url" rel="index">
                    <span itemprop="name">命令词</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2021/06/29/命令词/命令词论文笔记（二）/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          
            <span class="post-meta-divider">|</span>
            <span class="page-pv"><i class="fa fa-file-o"></i>  阅读数
            <span class="busuanzi-value" id="busuanzi_value_page_pv" ></span>
            </span>
          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="使用Google-Speech-Commands-dataset的论文"><a href="#使用Google-Speech-Commands-dataset的论文" class="headerlink" title="使用Google Speech Commands dataset的论文"></a>使用Google Speech Commands dataset的论文</h1><ul>
<li>Google speech commands dataset：谷歌开源唤醒词数据集，有6万条，30个唤醒词，每条音频长度1s</li>
<li>Speech commands dataset v1  下载地址：<a target="_blank" rel="noopener" href="http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz">http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz</a>  </li>
<li>Speech commands dataset v2  下载地址：<a target="_blank" rel="noopener" href="http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz">http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz</a> 或 <a target="_blank" rel="noopener" href="https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz">https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz</a></li>
<li>官方例子：<a target="_blank" rel="noopener" href="https://www.tensorflow.org/tutorials/audio/simple_audio">https://www.tensorflow.org/tutorials/audio/simple_audio</a></li>
<li>输入t帧（其实就是1s），输出1个类别向量（每个类是一个关键词（yes、no、on、off……））</li>
</ul>
<h2 id="paper、code目录"><a href="#paper、code目录" class="headerlink" title="paper、code目录"></a>paper、code目录</h2><blockquote>
<p>Sainath, Tara N., and Carolina Parada. “Convolutional neural networks for small-footprint keyword spotting.” <em>Sixteenth Annual Conference of the International Speech Communication Association</em>. 2015</p>
<p>github开源代码：<a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands</a></p>
</blockquote>
<blockquote>
<p>Choi S ,  Seo S ,  Shin B , et al. Temporal Convolution for Real-time Keyword Spotting on Mobile Devices[J].  2019</p>
<p>github开源代码：<a target="_blank" rel="noopener" href="https://github.com/hyperconnect/TC-ResNet">https://github.com/hyperconnect/TC-ResNet</a></p>
</blockquote>
<blockquote>
<p>de Andrade, Douglas Coimbra, et al. “A neural attention model for speech command recognition.” <em>arXiv preprint arXiv:1808.08929</em> (2018).</p>
<p>github开源代码：<a target="_blank" rel="noopener" href="https://github.com/douglas125/SpeechCmdRecognition">https://github.com/douglas125/SpeechCmdRecognition</a></p>
</blockquote>
<blockquote>
<p>Vygon, Roman, and Nikolay Mikhaylovskiy. “Learning efficient representations for keyword spotting with triplet loss.” <em>International Conference on Speech and Computer</em>. Springer, Cham, 2021</p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/roman-vygon/triplet_loss_kws">Github: Learning Efficient Representations for Keyword Spotting with Triplet Loss</a> pytorch、nemo</p>
</blockquote>
<blockquote>
<p>Shan C, Zhang J, Wang Y, et al. Attention-based end-to-end models for small-footprint keyword spotting[J]. arXiv preprint arXiv:1803.10916, 2018.</p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/isadrtdinov/kws-attention">https://github.com/isadrtdinov/kws-attention</a> 网友的复现代码</p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/Kirili4ik/kws-attention-pytorch">https://github.com/Kirili4ik/kws-attention-pytorch</a> 网友的复现代码</p>
</blockquote>
<blockquote>
<p>D. Seo, H. -S. Oh and Y. Jung, “Wav2KWS: Transfer Learning From Speech Representations for Keyword Spotting,” in IEEE Access, vol. 9, pp. 80682-80691, 2021, doi: 10.1109&#x2F;ACCESS.2021.3078715.</p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/qute012/Wav2Keyword">https://github.com/qute012/Wav2Keyword</a></p>
</blockquote>
<blockquote>
<p>Howl: A Deployed, Open-Source Wake Word Detection System</p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/castorini/howl">https://github.com/castorini/howl</a> pytorch</p>
</blockquote>
<blockquote>
<p>BYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation</p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/nttcslab/byol-a">https://github.com/nttcslab/byol-a</a> pytorch</p>
</blockquote>
<blockquote>
<p>Stochastic Adaptive Neural Architecture Search for Keyword Spotting</p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/TomVeniat/SANAS">https://github.com/TomVeniat/SANAS</a> pytorch</p>
</blockquote>
<blockquote>
<p>Keyword Transformer: A Self-Attention Model for Keyword Spotting</p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/ARM-software/keyword-transformer">https://github.com/ARM-software/keyword-transformer</a> tensorflow</p>
</blockquote>
<hr>
<blockquote>
<p>&#x3D;&#x3D;Sainath, Tara N., and Carolina Parada. “Convolutional neural networks for small-footprint keyword spotting.” <em>Sixteenth Annual Conference of the International Speech Communication Association</em>. 2015. citation：383&#x3D;&#x3D; 谷歌的论文</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/jialilian5181/article/details/83787886">论文翻译：Convolutional Neural Networks for Small-footprint Keyword Spotting</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/speech_commands">开源代码</a>，在TensorFlow官网可以下载</p>
</blockquote>
<ul>
<li>和Deep Kws做法一致，将DNN换成CNN</li>
<li>CNN输入：time*frequency，strides filters in frequency ，pools in time </li>
<li>尝试了不同下采样、pooling</li>
</ul>
<hr>
<blockquote>
<p>&#x3D;&#x3D;Shan C, Zhang J, Wang Y, et al. Attention-based end-to-end models for small-footprint keyword spotting[J]. arXiv preprint arXiv:1803.10916, 2018.&#x3D;&#x3D;</p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/isadrtdinov/kws-attention">https://github.com/isadrtdinov/kws-attention</a> 网友的复现代码</p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/Kirili4ik/kws-attention-pytorch">https://github.com/Kirili4ik/kws-attention-pytorch</a> 网友的复现代码</p>
</blockquote>
<h4 id="思想"><a href="#思想" class="headerlink" title="思想"></a>思想</h4><p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210820000810717.png" alt="image-20210820000810717"></p>
<ul>
<li>用attention机制的模型做命令词模型，没有事先训asr模型，输入T帧fbank语音特征，输出一个命令词概率，高于阈值就是命令词，是end2end，</li>
<li>用CRNN&#x2F;GRU做encoder，输入fbank feature，输出high level feature（效果CRNN&gt;GRU&gt;LSTM）</li>
<li>CRNN性能好，但是有1.5s延时，还是GRU更合适。</li>
<li>attention用soft attention和average attention（效果soft attention&gt;average attention）</li>
<li>减少计算量：滑动窗口时，每次滑动会得到一个T帧对应label概率，此时只要计算新添加的帧的计算，重叠部分不计算</li>
<li>运行时窗长都是100帧地执行，计算量小。[可改进之处]：当检测到keyword时（即当前帧超过给定的阈值，触发），那么就把窗长变为189帧，检测是否为keyword。</li>
<li>用20个滑动输出分类结果做smooth</li>
</ul>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210820000826516.png" alt="image-20210820000826516"></p>
<ul>
<li><ul>
<li><input checked disabled type="checkbox"> 滑窗这个要好好想想怎么实现？答：雷博说：把倒一层构成c的每个α和h时间步的值都保留下来，然后下一帧来了，就可以只算下一帧的，然后再softmax计算。</li>
</ul>
</li>
</ul>
<h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><ul>
<li>baseline用Deep KWS</li>
<li>网络结构：CRNN网络，一层CNN层，两层RNN层(64个节点)</li>
<li>训练集：正样本188.9k, 负样本 1007.4k</li>
<li>验证集：正样本9.9k, 负样本 53k</li>
<li>测试集：正样本 28.8k , 负样本32.8k</li>
<li>输入特征：PCEN特征。每条音频持续时间1.9 seconds.</li>
<li>four-syllable Mandarin Chinese term (“xiao-ai-tong-xue”)  ，∼188.9K positive examples (∼99.8h) and ∼1007.4K  negative examples (∼1581.8h) as the training set.   正样本：负样本&#x3D;10：1</li>
</ul>
<h4 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h4><ul>
<li>∼84K parameters </li>
<li>1.02% FRR at 1.0 FA&#x2F;hour.</li>
</ul>
<hr>
<blockquote>
<p>&#x3D;&#x3D;Bai Y, Yi J, Tao J, et al. A Time Delay Neural Network with Shared Weight Self-Attention for Small-Footprint Keyword Spotting[C]&#x2F;&#x2F;INTERSPEECH. 2019: 2190-2194&#x3D;&#x3D;.</p>
</blockquote>
<h4 id="思想-1"><a href="#思想-1" class="headerlink" title="思想"></a>思想</h4><ul>
<li>用TDNN-attention结构做kws，输入音频特征，输出分类结果</li>
<li>特征量很小（12K）</li>
<li>没用RNN，用TDNN（前馈网络），减少计算量，支持并行</li>
<li>attention用的shared weight self-attention，就是Q、K、V本来由三个$w_ix+b_i$而来（不同project投影），现在是乘以同一个wx+b，最后是multi-head</li>
<li>输入特征做下采样（tdnn povey原始论文也做的）</li>
<li>attention层后接两层tdnn，后接 global average</li>
<li>多帧输出-&gt;1个输出</li>
</ul>
<p><strong>模型结构为</strong>：</p>
<img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210608174746396.png" alt="image-20210608174746396" style="zoom: 80%;">

<p>global average 做了polling，变成一个向量</p>
<p>最后softmax输出是只有一个向量</p>
<p><strong>下采样为</strong>：</p>
<img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210608174813294.png" alt="image-20210608174813294" style="zoom: 67%;">



<h4 id="实验-1"><a href="#实验-1" class="headerlink" title="实验"></a>实验</h4><ul>
<li>数据集用Google Speech Commands  ，都是1s，一个word的音频</li>
<li>10个命令词，20个filler（归为一个label）</li>
<li>用分类错误率作为评价指标</li>
</ul>
<blockquote>
<p>参考：<a target="_blank" rel="noopener" href="https://blog.csdn.net/chenxi910911/article/details/102615068?spm=1001.2014.3001.5501">A Time Delay Neural Network with Shared Weight Self-Attention for Small-Footprint Keyword Spotting浅析</a></p>
</blockquote>
<p>中心思想：通过共享自注意力机制的权重，在维持性能不变的情况下，减少模型参数<br>本文的创新点：一是，用前馈神经网络代替在注意力机制中常用的回归神经网络，加速模型计算[用tdnn代替rnn]；二是，自注意力机制中的三个矩阵进行参数共享，减少模型参数<br>文章中提到的技术：TDNN、self-attention、SWSA(Shared-Weight Self-Attention)<br>TDNN技术：获取序列的局部特征<br>self-attention技术：用三个不同的权重矩阵将特征映射到不同空间中、获取序列的全局特征<br>shared-weight self-attention技术：用同一个权重矩阵将特征映射到同一空间中、减少模型参数。</p>
<p>模型结构：第一层TDNN-SUB（TDNN降采样层），实现方法：采用滑动窗的方式，在输入层矩阵T<em>in</em>上设置一个宽度W为3（通常根据第一层拼帧结构决定）的滑动窗，当步长K超过1时（步长不易超过窗长），达到了降采样的效果，维度D<em>out</em>减少为(T<em>in</em> − w + 1)&#x2F;k向上取整</p>
<p>第二层是SWSA（权重共享的自注意力机制），也是本文的重点，详细结构见F1（a）右手边虚框<br>第一步：输入为U，共享矩阵为W，Vi&#x3D;WU，Vi为自注意力机制的输入，Vi是通过自注意力机制将输入特征U映射到某一空间得到的特征<br>第二步：由于权重共享，原来的Vq×Vk也就是现在的V×（V转置）即矩阵与矩阵本身点乘；<br>第三步：第二步得到的矩阵进行scale操作&#x3D;&#x3D;（scale操作的目的：是为了防止矩阵上的值累加到很大的值，梯度更新过程中出现梯度消失的现象）通常是将矩阵上的值除以矩阵维度的开方&#x3D;&#x3D;；<br>第四步：计算注意力得分，经过softmax层，对矩阵的每一行进行一个softmax规整，即矩阵的每一行的值加和为1；<br>第五步：将注意力得分与V点乘得到新的矩阵输出后紧跟RELU和LayerNorm层，最终得到SWSA层的输出<br>第三层和第四层是TDNN层<br>第五层：&#x3D;&#x3D;globalAverage层：CNN中有类似的全局平均池化层&#x3D;&#x3D;<br>在这边文章中未提及，但通过后面的实验章节推测，是权重矩阵每一列求平均值最终输出一个与当前层输入矩阵相同维度的向量</p>
<p>下面是权重共享后的self-attention公式：</p>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/20191017213712901.png" alt="在这里插入图片描述"></p>
<p>multi-head self-attention版本</p>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/20191017214108129.png" alt="在这里插入图片描述"></p>
<p>其中i的值等于共享权重矩阵的个数，当i为1是就是上述描述的权重共享self-attention机制；当i大于1是此时的共享权重是i个（类似于CNN有i个卷积核的概念），当i大于1时，权重矩阵的维度就会缩减为原来的1&#x2F;i,通常为整数，其他步骤与上述的self-attention步骤一致，在self-attention输出层后增加一层将muli-head self-attention的输出拼接到一起，乘一个矩阵得到与i为1时相同大小的矩阵</p>
<img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/20191017215828482.jpg" alt="在这里插入图片描述" style="zoom:50%;">



<hr>
<blockquote>
<p>&#x3D;&#x3D;Choi S ,  Seo S ,  Shin B , et al. Temporal Convolution for Real-time Keyword Spotting on Mobile Devices[J].  2019. citation：41&#x3D;&#x3D;</p>
<p>论文开源代码：<a target="_blank" rel="noopener" href="https://github.com/hyperconnect/TC-ResNet">https://github.com/hyperconnect/TC-ResNet</a>  </p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_37598106/article/details/105518851">Lebhoryi的csdn笔记TC-ResNet</a></p>
<p><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/83e159170dbb">实时语音唤醒–Temporal Convolution for Real-time Keyword Spotting on Mobile Devices</a></p>
</blockquote>
<h4 id="思路"><a href="#思路" class="headerlink" title="思路"></a>思路</h4><ul>
<li><p>评测目标之一是测量在移动设备上的实际延迟 latency；</p>
</li>
<li><p>采用一维卷积，一维卷积优点：不对频率维度卷积，能具有更大的感受野（对于特征是语谱图输入的一些模型来说），同时获取高频和低频信息，计算量减少；</p>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210824101219809.png" alt="image-20210824101219809"></p>
</li>
</ul>
<h4 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h4><ul>
<li><p>输入MFCC，mfcc维度作为通道数c</p>
</li>
<li><p>使用一个宽度系数width multiplier，用来增加或减少每一层的通道数c，比如原来是{16,24,32,48}，乘1.5&#x3D;{24,36,48.72}，命名为TC-ResNet8-1.5</p>
</li>
<li><p>使用ResNet结构，cnn卷积核m *1（而不是3 *3），第一层m&#x3D;3，其他层m&#x3D;9</p>
</li>
<li><p>没有bias，bn层的作用是scale&amp;shift</p>
</li>
<li><p>网络结构：（其中k是width multiplier倍乘系数）</p>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210824101255801.png" alt="image-20210824101255801"></p>
</li>
</ul>
<h4 id="实验-2"><a href="#实验-2" class="headerlink" title="实验"></a>实验</h4><ul>
<li><p>实验结果考核了accuracy、ROC、耗时&#x3D;&#x3D;FLOPs&#x3D;&#x3D;、Params</p>
</li>
<li><p>TensorFlow Lite Android benchmark tool 判断推理时间的好坏</p>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210824101358185.png" alt="image-20210824101358185"></p>
</li>
</ul>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210824101413415.png" alt="image-20210824101413415"></p>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/12976336-3796810af0e64f2b.png" alt="12976336-3796810af0e64f2b"></p>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/12976336-003886915241671f.png" alt="12976336-003886915241671f"></p>
<img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/20200414190510809.png" alt="img">

<hr>
<blockquote>
<p>&#x3D;&#x3D;Mittermaier S, Kürzinger L, Waschneck B, et al. Small-footprint keyword spotting on raw audio data with sinc-convolutions[C]&#x2F;&#x2F;ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020: 7454-7458.  citation:14&#x3D;&#x3D;</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_37598106/article/details/105540840">Lebhoryi的csdn笔记</a></p>
</blockquote>
<h4 id="思路-1"><a href="#思路-1" class="headerlink" title="思路"></a>思路</h4><ul>
<li>用SincNet直接读取原始音频进行训练</li>
</ul>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210824110540981.png" alt="image-20210824110540981"></p>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20210824110631141.png" alt="image-20210824110631141"></p>
<h4 id="结果-1"><a href="#结果-1" class="headerlink" title="结果"></a>结果</h4><ul>
<li>sincConv+GDSConv模型的accuracy（97.3%）比TC-ResNet（96.6%）更高；参数量（62k）比TC-ResNet（305k）更少；</li>
</ul>
<hr>
<blockquote>
<p>&#x3D;&#x3D;Chen X, Yin S, Song D, et al. Small-footprint keyword spotting with graph convolutional network[C]&#x2F;&#x2F;2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019: 539-546. citation：6&#x3D;&#x3D;</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_37598106/article/details/105560949">Lebhoryi的csdn笔记CENet-GCN (2019年)</a></p>
</blockquote>
<h4 id="思路-2"><a href="#思路-2" class="headerlink" title="思路"></a>思路</h4><ul>
<li>使用了图神经网络GCN</li>
<li>结构紧凑</li>
</ul>
<h4 id="结果-2"><a href="#结果-2" class="headerlink" title="结果"></a>结果</h4><p>- </p>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/JkoL7t.png" alt="img"></p>
<hr>
<p>【重要】</p>
<blockquote>
<p>&#x3D;&#x3D;Rybakov O, Kononenko N, Subrahmanya N, et al. Streaming keyword spotting on mobile devices[J]. arXiv preprint arXiv:2005.06720, 2020. citations：15&#x3D;&#x3D; 谷歌的论文</p>
<p>github开源代码：<a target="_blank" rel="noopener" href="https://github.com/google-research/google-research/tree/master/kws_streaming">https://github.com/google-research/google-research/tree/master/kws_streaming</a></p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_37598106/article/details/106801481">Lebhoryi的csdn笔记stream_kws_cnn</a></p>
</blockquote>
<h4 id="思路-3"><a href="#思路-3" class="headerlink" title="思路"></a>思路</h4><ul>
<li>stream流式 卷积也可以流式，意味着不用输入定长才有输出，输入任意一段（甚至只要给1帧）就会有输出，要研究代码，看是否可以一个vad长度输入，得到一个输出</li>
<li></li>
</ul>
<hr>
<p>A survey on structured discriminative spoken keywordspotting  </p>
<hr>
<blockquote>
<p>&#x3D;&#x3D;de Andrade, Douglas Coimbra, et al. “A neural attention model for speech command recognition.” <em>arXiv preprint arXiv:1808.08929</em> (2018).&#x3D;&#x3D;</p>
<p>github开源代码：<a target="_blank" rel="noopener" href="https://github.com/douglas125/SpeechCmdRecognition">https://github.com/douglas125/SpeechCmdRecognition</a></p>
</blockquote>
<hr>
<blockquote>
<p>&#x3D;&#x3D;Yang, Chen, Xue Wen, and Liming Song. “Multi-Scale Convolution for Robust Keyword Spotting.” <em>INTERSPEECH</em>. 2020.&#x3D;&#x3D;三星研究院</p>
<p>线上会议video：<a target="_blank" rel="noopener" href="http://www.interspeech2020.org/index.php?m=content&amp;c=index&amp;a=show&amp;catid=321&amp;id=835">http://www.interspeech2020.org/index.php?m=content&amp;c=index&amp;a=show&amp;catid=321&amp;id=835</a></p>
</blockquote>
<h3 id="PPT"><a href="#PPT" class="headerlink" title="PPT"></a>PPT</h3><ul>
<li>HMM在噪声环境下表现较差</li>
</ul>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104100737871.png" alt="image-20220104100737871"></p>
<img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104100921735.png" alt="image-20220104100921735" style="zoom:80%;">



<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104100945713.png" alt="image-20220104100945713"></p>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104101011948.png" alt="image-20220104101011948"></p>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104101029737.png" alt="image-20220104101029737"></p>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104101052521.png" alt="image-20220104101052521"></p>
<img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104101107772.png" alt="image-20220104101107772">



<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104101142998.png" alt="image-20220104101142998"></p>
<h3 id="思路-4"><a href="#思路-4" class="headerlink" title="思路"></a>思路</h3><ul>
<li>实现低功耗small footprint：通过使用depthwise-separable convolutions in a ResNet framework；</li>
<li>实现噪声鲁棒性：通过multi-scale ensemble of classifiers；每个分类器处理不同的输入特征，同时通过大量的参数共享让size紧凑；</li>
<li>多个depthwise-separable residual unit (DRU)   堆叠起来的，用现在最流行的depthwise-separable cnn（减少参数量），然后加上resnet；</li>
<li>model compression in <strong>DRU</strong> 具体实现过程：Incoming 𝑛 -channel feature map is down-projected to 𝑛&#x2F;2 channels by 1 × 1 convolution, processed per-channel by 3 × 3, up-projected to 𝑛 channels by another 1 × 1, then summed with original input to complete the residual unit；<ol>
<li>一开始正常1* 1卷积滤波器个数n&#x2F;2，参数量 1* 1* n* n&#x2F;2</li>
<li>然后depthwise conv（channel个数n&#x2F;2），参数量 3* 3* n&#x2F;2</li>
<li>最后1* 1 pointwise卷积（其实也是普通卷积）滤波器个数n，参数量 1* 1* n&#x2F;2* n</li>
</ol>
</li>
</ul>
<img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104111436716.png" alt="image-20220104111436716" style="zoom:80%;">



<ul>
<li><p>Multi-scale ensemble  具体实现过程：</p>
<p>在不同的层，不同的t，不同的持续时间窗口w（d,t,w），抽取output，经过变换矩阵softmax，输出某个head的结果，每个head的帧取的时间间隔不同，比如第一个head是取0.8s，帧移0.4s取，第二个head是取1s，帧移0.5s取；</p>
<p>不同head所经过的变换矩阵里面的权重参数是一样的（经过同一个变换矩阵）（weight sharing），这是为了结构紧凑；</p>
<p>分类是数帧进行average pooling，再分类</p>
</li>
<li><p>loss function：$naiveCE:&#x3D;\sum_{d,t,w}logp_{d,t,w}(y)$</p>
</li>
</ul>
<img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220104145324460.png" alt="image-20220104145324460" style="zoom:80%;">

<ul>
<li>分类判断 $ \large{result&#x3D;\left{<br>\begin{aligned}<br>arg\max_{v\in{v}}(\max\limits_{d,t,w}p_{d,t,w}(v)),if{\exists}p_{d,t,w}(v)&gt;th \<br>nonkeyword, otherwise<br>\end{aligned}<br>\right.}$</li>
</ul>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><ul>
<li>10 conv layers in total, including one initial convolution and 9 stacked DRUs  </li>
<li>Initial convolution reduces input size and projects features into multiple channels  </li>
<li>The 9 DRUs are arranged in three groups. DRUs in the same group have the same number of channels. Deeper groups are allocated more channels to learn more complex features</li>
</ul>
<hr>
<blockquote>
<p>&#x3D;&#x3D;Li, Ximin, Xiaodong Wei, and Xiaowei Qin. “Small-Footprint Keyword Spotting with Multi-Scale Temporal Convolution.” <em>arXiv preprint arXiv:2010.09960</em> (2020).&#x3D;&#x3D;</p>
</blockquote>
<h3 id="思路-5"><a href="#思路-5" class="headerlink" title="思路"></a>思路</h3><ul>
<li><p>一维卷积，time是长，没有宽，宽是1，特征维度（频率）是channel，这种卷积叫temporal convolution；</p>
</li>
<li><p>提出temporal efficient neural network (TENet)  ，主要结构是inverted bottleneck block (IBB)  ，该结构两头小中间大（和普通的bottleneck相反，因此叫inverted bottleneck），两头是1* 1卷积（沿着channel卷）（pointwise），中间是不沿着channel卷积的9*1卷积（depthwise），和普通的depthwise separable conv顺序相反。</p>
<p>IBB里的第一个1×1卷积的目的是通过扩展channel的数量，将input嵌入（embed）到高维子空间中；</p>
<p>IBB里的depthwise卷积是temporal卷积，通过对每个输入channel应用一个卷积滤波器，和非线性变换，来实现轻量级滤波；（注意这里只是depthwise conv不是depthwise seperate conv）</p>
<p>IBB里的最后一个1×1卷积将tensor转换回低维compact子空间，用于channels间的信息传输；</p>
</li>
</ul>
<img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220105142223167.png" alt="image-20220105142223167" style="zoom: 50%;">



<ul>
<li>末尾层 average pooling -&gt; fully connected -&gt; softmax</li>
</ul>
<img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220105142911152.png" alt="image-20220105142911152" style="zoom:50%;">

<ul>
<li><p>提出Multi-branch Temporal Convolution Module，目的是为了捕捉短期和长期时间信息特征；</p>
<p>具体实现过程：让每个branch的kernel size都不同，从而帮助branch从它的时间粒度中学习不一样的模式；</p>
<p>多尺度融合（不同kernel size后的特征融合）：element-wise add（各个位置的元素等于两&#x2F;N个输入矩阵相同位置元素的乘积的矩阵，再加和） (Hadamard product)</p>
<p>在训练时，将TENet中的所有depthwise卷积层都替换为MTConvs（IBB里独一个depthwise conv，替换成kernel size不同的多个depthwise conv（最后element-wise），叫做multi-scale））</p>
</li>
</ul>
<img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220105142956043.png" alt="image-20220105142956043" style="zoom:50%;">

<p>​	把下图原本只有一个的卷积，替换成多个不同kernel size卷积加和（上图）</p>
<ul>
<li><p>kernel fusion of MTConv </p>
<p>$\large{O_{t,1,j}&#x3D;(\sum\limits^k_{i&#x3D;-k}M_{t+i,1,j}F_{i+k+1,1,j}-u_j)\frac{\gamma_j}{\sigma_j}+\beta_j}$</p>
<p>其中，M是输入，F是卷积核，$k&#x3D;\frac{D-1}{2}$，D是卷积核尺寸（D×1×C），$u_j$和$\sigma_j$是BN层的channel-wise均值和标准差，$\gamma_j$和$\beta_j$是scaling和shifting系数（可训练 更新）；</p>
</li>
</ul>
<hr>
<blockquote>
<p>&#x3D;&#x3D;Vygon, Roman, and Nikolay Mikhaylovskiy. “Learning efficient representations for keyword spotting with triplet loss.” <em>International Conference on Speech and Computer</em>. Springer, Cham, 2021.&#x3D;&#x3D;ciations：12</p>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/roman-vygon/triplet_loss_kws">Github: Learning Efficient Representations for Keyword Spotting with Triplet Loss</a></p>
</blockquote>
<h3 id="思路-6"><a href="#思路-6" class="headerlink" title="思路"></a>思路</h3><ul>
<li><p>triplet loss：$\large l(p_i,p_i ^+,p_i ^- &#x3D; {0, g + D(f(p_i), f(p_i ^+)) - D(f(p_i), f(p_i ^-))})$</p>
<p>其中，$p_i$ 是anchor image，$p_i ^+$ 是positive image， $p_i ^-$ 是negative image，$g$ 是gap parameter regularizes the gap between the distance of the two image pairs: $(p_i,p_i ^+)$ and $(p_i,p_i ^-)$，D可以是欧氏距离；</p>
<p>这里是用来提embedding用的，输入A样本特征，输出embedding（高级特征），让A的embedding和同一个batch里的同类embedding距离近，让A的embedding和同一个batch里的不同类embedding距离远；</p>
<p>这里的triplet loss用来训练一个 <strong>triplet encoder</strong></p>
</li>
<li><p>提出triplet-loss based metric embeddings   + KNN分类器；实验优于ce，达到Google Speech Commands dataset  的SOTA（98.55% (V1 12分类(10keyword+sil+unknown)) 、98.37% (V2 12分类) ）；</p>
</li>
<li><p>提出基于音素相似度的batch sampling方法，改善了当数据不平衡时的F1</p>
</li>
<li><p>&#x3D;&#x3D;论文中做实验表明RNN-based的模型用上triplet loss会变差&#x3D;&#x3D;</p>
</li>
<li><p>先用模型提特征，把特征用KNN进行分类</p>
</li>
<li><p>batch sampling：Phonetic: Calculate a matrix of phonetic similarity for all the words in the dataset, sample batch_size&#x2F;2 classes, then, for each sampled class add three random phonetically similar words (equally distributed) to the batch. Similarity score is calculated using SoundEx, Caverphone, Metaphone and NYSIIS algorithms；在一个batch里，相似的音多采样一点，triplet loss才比较有效，不然已经很不相似了，本身就很不像，不怎么需要区分开；</p>
</li>
</ul>
<h3 id="模型-1"><a href="#模型-1" class="headerlink" title="模型"></a>模型</h3><ul>
<li>基于Honk框架，直接用了（honk框架和wekws一样是框架），提高级特征（embedding）</li>
</ul>
<img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220331115729019.png" alt="image-20220331115729019" style="zoom:67%;">

<img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220331115825729.png" alt="image-20220331115825729" style="zoom: 67%;">

<img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220331123332076.png" alt="image-20220331123332076" style="zoom:67%;">

<h3 id="实验-3"><a href="#实验-3" class="headerlink" title="实验"></a>实验</h3><ul>
<li>做了两组实验：</li>
</ul>
<ol>
<li>Google Speech Commands dataset；</li>
<li>LibriWords Datasets  这是用librispeech经过,the Montreal Forced Aligner(“Montreal forced aligner:Trainable text-speech alignment using kaldi” ) 对齐，得到word边界，进行实验，目的是查看用triplet loss在数据更大，分类数更多的情况效果能有多好  ；尝试了LibriWords10, LibriWords100, LibriWords1000, LibriWords10000</li>
</ol>
<h3 id="结果-3"><a href="#结果-3" class="headerlink" title="结果"></a>结果</h3><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220331123319620.png" alt="image-20220331123319620" style="zoom:67%;">

<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><blockquote>
<p>github：<a target="_blank" rel="noopener" href="https://github.com/roman-vygon/triplet_loss_kws">https://github.com/roman-vygon/triplet_loss_kws</a></p>
</blockquote>
<p>requirement：安装nemo、nemo-asr、apex、protobuf&#x3D;&#x3D;3.9.2，安装如下：</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装nemo</span></span><br><span class="line">pip install nemo-toolkit[all]</span><br><span class="line">pip install nemo_toolkit==0.10.1？</span><br><span class="line">pip install nemo_toolkit[asr]？</span><br><span class="line"><span class="comment"># 安装protobuf：</span></span><br><span class="line">pip install protobuf==3.9.2</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装apex 直接用pip install apex安装后会报错</span></span><br><span class="line">git <span class="built_in">clone</span> https://github.com/NVIDIA/apex</span><br><span class="line"><span class="built_in">cd</span> apex</span><br><span class="line">pip install -v --disable-pip-version-check --no-cache-dir ./</span><br><span class="line"><span class="comment">#pip install -v --disable-pip-version-check --no-cache-dir --global-option=&quot;--cpp_ext&quot; --global-option=&quot;--cuda_ext&quot; ./</span></span><br></pre></td></tr></table></figure>





<hr>
<blockquote>
<p>&#x3D;&#x3D;Kim, Byeonggeun, et al. “Broadcasted residual learning for efficient keyword spotting.” <em>arXiv preprint arXiv:2106.04140</em> (2021).&#x3D;&#x3D;</p>
<p>github：&#x2F;home&#x2F;data&#x2F;yelong&#x2F;google-research&#x2F;kws_streaming&#x2F;models&#x2F;bc_resnet.py</p>
<p>github网友复现代码：<a target="_blank" rel="noopener" href="https://github.com/roman-vygon/BCResNet">https://github.com/roman-vygon/BCResNet</a></p>
</blockquote>
<h3 id="思路-7"><a href="#思路-7" class="headerlink" title="思路"></a>思路</h3><ul>
<li>提出broadcasted residual learning方法、broadcasted-residual connection  、Broadcasting-residual network  (BC-ResNet)</li>
</ul>
<p><img src="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/image-20220426151746370.png" alt="image-20220426151746370"></p>
<ul>
<li><p>Broadcasted Residual Learning  :在残差结构$y&#x3D;x+f(x)$上改进为：$\large y&#x3D;x+BC(f_1(avgpool(f_2(x))))$</p>
<p>其中，$f_1$是时间维度的函数，$f_2$是时间、频率二维图；</p>
<p>$f_2$ 输出也是二维的，然后对频率轴进行平均池化，得到时间、channel轴，再进入$f_1$函数，相当于$f_2$是一个提取不同帧时间&#x2F;不同高低频率之间相关性的一个特征抽取器（我们之前拿frequency作为channel，这里是单独一个维度、（channel有它自己一个维度）），经过$f_1$后又扩展成2D；在每个residual block里重复这种平均、扩展操作；称之为Broadcasted Residual Learning  </p>
</li>
<li><p>BC-ResNet Block ：$\large y&#x3D;x+f_2(x)+ BC(f_1(avgpool(f_2(x))))$</p>
<p>流程：（描述时忽略channel通道）$x\in R^{h\times w}$  ,h是frequency，w是time，做3* 1的frequency-depthwise卷积（$f_2$），就是沿着频率h轴做depthwise卷积（滤波器数量和frequency一致，不求和，kerenl size&#x3D;3），做SSN归一化，沿着频率轴做平均池化，再沿着时间轴w做1* 3time-depthwise卷积（$f_1$）（滤波器数量和time一致，不求和，kerenl size&#x3D;3），接BN层，swish激活函数，再接1* 1 pointwise卷积，提取时间帧前后关系信息，再沿着channel轴做dropout，输出 $y\in R^{h\times w}$</p>
</li>
<li><p>SubSpectral Normalization (SSN)  ：将输入频率分成多组，分别做归一化；用SSN代替BN层，来获取frequency-aware temporal features</p>
</li>
</ul>
<p>12分类，训练时unknown和silence label的比例为10、10；40维mfcc 30ms帧长10ms帧移；time shift [-100,100]，概率0.8、0.8比例加噪、spec augment 只用two time 和two frequency masks，没用time wraping</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E5%91%BD%E4%BB%A4%E8%AF%8D/" rel="tag"># 命令词</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%94%EF%BC%89/" rel="next" title="命令词论文笔记（五）区分性训练用在命令词的论文">
                <i class="fa fa-chevron-left"></i> 命令词论文笔记（五）区分性训练用在命令词的论文
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B9%9D%EF%BC%89/" rel="prev" title="命令词论文笔记（九）Spoken Term Detection">
                命令词论文笔记（九）Spoken Term Detection <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a target="_blank" rel="noopener" href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name"></p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%7C%7C%20archive">
              
                  <span class="site-state-item-count">203</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8Google-Speech-Commands-dataset%E7%9A%84%E8%AE%BA%E6%96%87"><span class="nav-number">1.</span> <span class="nav-text">使用Google Speech Commands dataset的论文</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#paper%E3%80%81code%E7%9B%AE%E5%BD%95"><span class="nav-number">1.1.</span> <span class="nav-text">paper、code目录</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%9D%E6%83%B3"><span class="nav-number">1.1.0.1.</span> <span class="nav-text">思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">1.1.0.2.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C"><span class="nav-number">1.1.0.3.</span> <span class="nav-text">结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%9D%E6%83%B3-1"><span class="nav-number">1.1.0.4.</span> <span class="nav-text">思想</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C-1"><span class="nav-number">1.1.0.5.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%9D%E8%B7%AF"><span class="nav-number">1.1.0.6.</span> <span class="nav-text">思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83"><span class="nav-number">1.1.0.7.</span> <span class="nav-text">训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C-2"><span class="nav-number">1.1.0.8.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%9D%E8%B7%AF-1"><span class="nav-number">1.1.0.9.</span> <span class="nav-text">思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C-1"><span class="nav-number">1.1.0.10.</span> <span class="nav-text">结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%9D%E8%B7%AF-2"><span class="nav-number">1.1.0.11.</span> <span class="nav-text">思路</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C-2"><span class="nav-number">1.1.0.12.</span> <span class="nav-text">结果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%9D%E8%B7%AF-3"><span class="nav-number">1.1.0.13.</span> <span class="nav-text">思路</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PPT"><span class="nav-number">1.1.1.</span> <span class="nav-text">PPT</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%9D%E8%B7%AF-4"><span class="nav-number">1.1.2.</span> <span class="nav-text">思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-number">1.1.3.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%9D%E8%B7%AF-5"><span class="nav-number">1.1.4.</span> <span class="nav-text">思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%9D%E8%B7%AF-6"><span class="nav-number">1.1.5.</span> <span class="nav-text">思路</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B-1"><span class="nav-number">1.1.6.</span> <span class="nav-text">模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C-3"><span class="nav-number">1.1.7.</span> <span class="nav-text">实验</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C-3"><span class="nav-number">1.1.8.</span> <span class="nav-text">结果</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81"><span class="nav-number">1.1.9.</span> <span class="nav-text">代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%9D%E8%B7%AF-7"><span class="nav-number">1.1.10.</span> <span class="nav-text">思路</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      
      
    </div>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=000000&w=1&t=n&d=x0EI09H6PL-1VTHANih6elfNIiKGL2U1VQlHq8todc4&co=000000&cmo=000000&cmn=000000&ct=000000'></script>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2025</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Long Ye</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.4</div>





        
<div class="busuanzi-count">
  <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      访客数
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      人
    </span>
  

  
    <span class="site-pv">
      总访问量
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      次
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  

    
      <script id="dsq-count-scr" src="https://yelong.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://example.com/2021/06/29/%E5%91%BD%E4%BB%A4%E8%AF%8D/%E5%91%BD%E4%BB%A4%E8%AF%8D%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/';
          this.page.identifier = '2021/06/29/命令词/命令词论文笔记（二）/';
          this.page.title = '命令词论文笔记（二）使用Google Speech Commands dataset的论文';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://yelong.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  














  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
